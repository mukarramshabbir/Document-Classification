{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Graph Theory Semester Project\n",
        "*Submitted To:* Mr. Waqas Ali  \n",
        "*Submitted By:*  \n",
        "\n",
        "- 2021-CS-58 Mukarram Ali\n",
        "- 2021-CS-59 Rayan Rasheed  \n",
        "\n"
      ],
      "metadata": {
        "id": "vL9ZAtEAANo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install networkx\n",
        "!pip install nltk\n",
        "!pip install scikit-learn\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install medium_api"
      ],
      "metadata": {
        "id": "u8j2pWdnDP5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries"
      ],
      "metadata": {
        "id": "CRgsntDP54MF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import os\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jsruv3x76KYA",
        "outputId": "c440233e-f221-4a87-fb78-babcc66e6d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scrape Data From World Wide Web"
      ],
      "metadata": {
        "id": "dnwZPeyW7PJB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python Scraper.py\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bHXRQPDA77us",
        "outputId": "7f001ec6-20b8-4e9b-9cf0-8d90d387fa10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Organizing the Scraping Data For Training"
      ],
      "metadata": {
        "id": "9cMS18fz9ez3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "folders = [\"Fashion and Beauty_data\", \"Health and Fitness_data\", \"Travel_data\"]\n",
        "all_data = []\n",
        "def read_text_files(folder_path, folder_name, max_files=15):\n",
        "    file_data = []\n",
        "    serial=1\n",
        "    category=folder_name.replace(\"_data\",\"\")\n",
        "    for idx, filename in enumerate(os.listdir(folder_path)):\n",
        "        if idx >= max_files:\n",
        "            break\n",
        "        if filename.endswith(\".txt\"):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                text = file.read()\n",
        "                file_data.append((serial, text, category))\n",
        "                serial+=1\n",
        "    return file_data\n",
        "for folder in folders:\n",
        "    folder_path = os.path.join(os.getcwd(), folder)\n",
        "    folder_data = read_text_files(folder_path, folder)\n",
        "    all_data.extend(folder_data)\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(all_data, columns=[\"Serial Number\", \"Text Data\", \"Category\"])\n",
        "\n",
        "# Write DataFrame to Excel\n",
        "output_excel_path = \"Data.xlsx\"\n",
        "df.to_excel(output_excel_path, index=False)"
      ],
      "metadata": {
        "id": "hH0sQ6CW9kiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Functions for MCS, Distance Matrix, Preprocess Data"
      ],
      "metadata": {
        "id": "vQqXPicY6a7u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSU0vfJ_DHvn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Preprocess text\n",
        "def preprocess_text(text):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Remove stop words and non-alphabetic tokens\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
        "    # Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Construct document graph\n",
        "def construct_document_graph(text):\n",
        "    tokens = preprocess_text(text)\n",
        "    G = nx.DiGraph()\n",
        "    for token in set(tokens):\n",
        "        G.add_node(token)\n",
        "    for i in range(len(tokens) - 1):\n",
        "        word1 = tokens[i]\n",
        "        word2 = tokens[i + 1]\n",
        "        if not G.has_edge(word1, word2):\n",
        "            G.add_edge(word1, word2)\n",
        "    return G\n",
        "\n",
        "# Compute maximal common subgraph (MCS)\n",
        "def maximal_common_subgraph(graph1, graph2):\n",
        "    matcher = nx.algorithms.isomorphism.GraphMatcher(graph1, graph2)\n",
        "    if matcher.subgraph_is_isomorphic():\n",
        "        isomorphisms = next(matcher.subgraph_isomorphisms_iter())\n",
        "        mcs = nx.Graph(graph1.subgraph(isomorphisms))\n",
        "        return mcs\n",
        "    else:\n",
        "        return nx.Graph()  # Return an empty graph if no MCS exists\n",
        "\n",
        "# Calculate distance measure based on MCS\n",
        "def mcs_distance(graph1, graph2):\n",
        "    num_vertices_graph1 = len(graph1.nodes)\n",
        "    num_edges_graph1 = len(graph1.edges)\n",
        "    num_vertices_graph2 = len(graph2.nodes)\n",
        "    num_edges_graph2 = len(graph2.edges)\n",
        "\n",
        "    # Compute MCS\n",
        "    mcs = maximal_common_subgraph(graph1, graph2)\n",
        "    num_vertices_mcs = len(mcs.nodes)\n",
        "    num_edges_mcs = len(mcs.edges)\n",
        "\n",
        "    # Calculate distance measure (e.g., dissimilarity ratio)\n",
        "    vertex_similarity = num_vertices_mcs / min(num_vertices_graph1, num_vertices_graph2)\n",
        "    edge_similarity = num_edges_mcs / min(num_edges_graph1, num_edges_graph2)\n",
        "\n",
        "    # You can combine vertex_similarity and edge_similarity to get an overall distance measure\n",
        "    # For example, you could take the average or maximum of the two\n",
        "\n",
        "    return 1 - (vertex_similarity + edge_similarity) / 2  # Return a dissimilarity measure\n",
        "def getDistancematric(doc_graph, documents, max_features):\n",
        "    distances = []\n",
        "    for train_doc_graph, _ in documents:\n",
        "        if train_doc_graph is not doc_graph:\n",
        "            distance = mcs_distance(doc_graph, train_doc_graph)\n",
        "            distances.append(distance)\n",
        "\n",
        "    # Pad or truncate the feature vector to ensure consistent dimensionality\n",
        "    padded_distances = distances[:max_features] + [0] * (max_features - len(distances))\n",
        "\n",
        "    return padded_distances\n",
        "\n",
        "\n",
        "def train_knn_classifier(documents, max_features, k=5):\n",
        "    X = []\n",
        "    y = []\n",
        "    for doc_graph, label in documents:\n",
        "        # Convert graph to feature vector using MCS distances\n",
        "        distancematric = getDistancematric(doc_graph, documents, max_features)\n",
        "        X.append(distancematric)\n",
        "        y.append(label)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X, y)\n",
        "\n",
        "    return knn\n",
        "\n",
        "\n",
        "def test_knn_classifier(knn_classifier, test_doc_graph, documents,max_features):\n",
        "    X_test = getDistancematric(test_doc_graph, documents,max_features)\n",
        "    predicted_label = knn_classifier.predict([X_test])[0]\n",
        "    return predicted_label\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Constructing graph from Scraped Data"
      ],
      "metadata": {
        "id": "b137sfXg5lcA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'Data.xlsx'\n",
        "data = pd.read_excel(file_path)\n",
        "page_data=data['Text Data'].tolist()\n",
        "genre_data=data['Category'].tolist()\n",
        "documents=[]\n",
        "for i in range(len(page_data)-5):\n",
        "  graph = construct_document_graph(page_data[i])\n",
        "  documents.append((graph,genre_data[i]))"
      ],
      "metadata": {
        "id": "wgaWXdbF5uLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check Document Format"
      ],
      "metadata": {
        "id": "nfLBFDZQxu-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnqpmmeVxxgU",
        "outputId": "ef8bb218-5479-44ee-b0a7-92dad4682294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(<networkx.classes.digraph.DiGraph object at 0x796829717250>, 'Fashion and Beauty'), (<networkx.classes.digraph.DiGraph object at 0x796828bedde0>, 'Fashion and Beauty'), (<networkx.classes.digraph.DiGraph object at 0x796828beddb0>, 'Fashion and Beauty'), (<networkx.classes.digraph.DiGraph object at 0x796828bedfc0>, 'Health and Fitness')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split Data for Training and Testing\n"
      ],
      "metadata": {
        "id": "_xTtvUjuxhWk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_docs, test_docs = train_test_split(documents, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "oXx2mcwrxlJK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training\n"
      ],
      "metadata": {
        "id": "zgUE0R7dxH4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train k-NN classifier\n",
        "max_features = 8\n",
        "knn_classifier = train_knn_classifier(train_docs, max_features, k=3)"
      ],
      "metadata": {
        "id": "9-Ha8R-ExNjK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing\n"
      ],
      "metadata": {
        "id": "ZQB8JpzVxWqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = []\n",
        "true_labels = []\n",
        "for test_graph, true_label in test_docs:\n",
        "    predicted_label = test_knn_classifier(knn_classifier, test_graph, train_docs, max_features)\n",
        "    predictions.append(predicted_label)\n",
        "    true_labels.append(true_label)\n",
        "print(predictions)\n",
        "print(true_labels)\n",
        "\n",
        "print(classification_report(true_labels, predictions))"
      ],
      "metadata": {
        "id": "GT4i9OCGxYEF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827b1b84-5df1-4288-816b-11deeb2f3625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Fashion and Beauty']\n",
            "['Fashion and Beauty']\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Fashion and Beauty       1.00      1.00      1.00         1\n",
            "\n",
            "          accuracy                           1.00         1\n",
            "         macro avg       1.00      1.00      1.00         1\n",
            "      weighted avg       1.00      1.00      1.00         1\n",
            "\n"
          ]
        }
      ]
    }
  ]
}